#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 09:28:48 2023

@author: speechlab
"""
import os
import requests
import sounddevice as sd
import soundfile as sf
import numpy as np
import torch
import torchaudio
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
from scipy.io.wavfile import write 
from transformers import pipeline
from flask import *
app = Flask(__name__)


app.static_folder = 'static'

@app.route('/')
def main():
   return render_template('index.html')
@app.route('/demo.html')
def demo():
   return render_template('demo.html')

@app.route('/asr.html')
def asr():
   return render_template('asr.html')


@app.route('/contact.html')
def contact():
   return render_template('contact.html')

@app.route('/index.html')
def index():
   return render_template('index.html')


@app.route('/projects.html')
def proj():
   return render_template('projects.html')


@app.route('/activities.html')
def act():
   return render_template('activities.html')


@app.route('/members.html')
def member():
   return render_template('members.html')


@app.route('/publications.html')
def pub():
    return render_template('publications.html')

token = "5ab9b1d788f7fe63b9f90b647b34dc6959877ef22c31b59b0c25a3a4074b8fe7"



@app.route('/update_language', methods=['GET', 'POST'])

def update_language():
    language = request.form['language']

    # Save the selected language to a file
    with open("lang.txt", "w") as outf:
        outf.write(language)

    # You can also perform any other actions with the selected language as needed

    # For simplicity, return a response indicating success
   

    return redirect(url_for("asr"))

@app.route("/asr_upload", methods=["GET", "POST"])
def asr_upload():
            f = request.files['file']
            f.save(f.filename)
            print(f.filename)
            f1 =f.filename.split('\\')
            file = f1[-1].split('.')[0]
            os.system("ch_wave " + f.filename + " -F 16000 -otype wav -o temp.wav")
            fp=open('lang.txt',"r")
            lines=fp.readlines()
            #language = g.languages
            # print(language)
            language = ''.join(lines)
            
            print(language)
            if language == 'hindi':
                wav_file_path = "temp.wav"
                processor = Wav2Vec2Processor.from_pretrained("theainerd/Wav2Vec2-large-xlsr-hindi")
                model = Wav2Vec2ForCTC.from_pretrained("theainerd/Wav2Vec2-large-xlsr-hindi")

                # Preprocessing function
                def preprocess_audio(audio_path):
                    speech_array, _ = torchaudio.load(audio_path)
                    # Apply any necessary preprocessing (e.g., resampling)
                    processed_input = processor(speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)
                    return processed_input

                # Preprocess audio
                processed_input = preprocess_audio(wav_file_path)

                # Model inference
                with torch.no_grad():
                    logits = model(processed_input.input_values, attention_mask=processed_input.attention_mask).logits

                # Decode predicted IDs
                predicted_ids = torch.argmax(logits, dim=-1)
                predicted = processor.batch_decode(predicted_ids)[0]

                # Print results
                #print(f"Audio Path: {wav_file_path}")
                print(f"Prediction: {predicted}")
                
            elif language == 'tamil':
                audio = "temp.wav"
                device = "cuda:0" if torch.cuda.is_available() else "cpu"

                transcribe = pipeline(task="automatic-speech-recognition", model="vasista22/whisper-tamil-small", chunk_length_s=30, device=device)

                transcribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language="ta", task="transcribe")
                predicted = transcribe(audio)["text"]
                print(predicted)
                #print('Transcription: ', transcribe(audio)["text"])
                #print(f"Audio Path: {wav_file_path}")
                
            
            
        # os.system("rm *.wav")
            return render_template('output_asr.html', transcript=predicted)
@app.route("/asr_record", methods=["GET", "POST"])
def asr_record():
        text = None
        # Recording parameters
        duration = 5  # seconds
        fs = 16000  # sample rate
        filename = 'temp.wav'

    # Record audio
        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=np.int16)
        sd.wait()
        print("recorded")
    # Save recorded audio to a file
        sf.write(filename, recording, fs)
        fp=open('lang.txt',"r")
        lines=fp.readlines()
        #language = g.languages
        # print(language)
        language = ''.join(lines)
        if language == 'hindi':
            wav_file_path = "temp.wav"
            processor = Wav2Vec2Processor.from_pretrained("theainerd/Wav2Vec2-large-xlsr-hindi")
            model = Wav2Vec2ForCTC.from_pretrained("theainerd/Wav2Vec2-large-xlsr-hindi")

            # Preprocessing function
            def preprocess_audio(audio_path):
                speech_array, _ = torchaudio.load(audio_path)
                # Apply any necessary preprocessing (e.g., resampling)
                processed_input = processor(speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors="pt", padding=True)
                return processed_input

            # Preprocess audio
            processed_input = preprocess_audio(wav_file_path)

            # Model inference
            with torch.no_grad():
                logits = model(processed_input.input_values, attention_mask=processed_input.attention_mask).logits

            # Decode predicted IDs
            predicted_ids = torch.argmax(logits, dim=-1)
            predicted = processor.batch_decode(predicted_ids)[0]

            # Print results
            #print(f"Audio Path: {wav_file_path}")
            print(f"Prediction: {predicted}")
            
        elif language == 'tamil':
            audio = "temp.wav"
            device = "cuda:0" if torch.cuda.is_available() else "cpu"

            transcribe = pipeline(task="automatic-speech-recognition", model="vasista22/whisper-tamil-small", chunk_length_s=30, device=device)

            transcribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language="ta", task="transcribe")
            predicted = transcribe(audio)["text"]
            print(predicted)
            #print('Transcription: ', transcribe(audio)["text"])
            #print(f"Audio Path: {wav_file_path}")
            
        
        
    # os.system("rm *.wav")
        return render_template('output_asr.html', transcript=predicted)

if __name__ == '__main__':
    app.run(host='0.0.0.0')